import pandas as pd
from typing import Optional, Dict

"""
HUDCrawler - Prototype static file crawler for HUD General datasets.
Will normalize common fields across different HUD general files (Income Limits, Assisted Units, LIHTC, MF properties).
Designed to be replaced by live API calls later to page HTML; for now it will read CSV/XLSX files statically.

Core normalized attributes included (not all must exist in source file):
- dataset_name, fiscal_year, state_name, state_code, county_name, fips_code, hud_geoid, msa_code, area_type,
  ami_median_family_income, ami_30_percent_limit, ami_50_percent_limit, ami_80_percent_limit, household_size,
  program_type, sub_program, total_units, occupied_units, vacancy_rate, assisted_units, funding_source,
  property_id, property_name, property_address, property_type, construction_year, owner_entity, managing_agent,
  hud_inspection_score, last_inspection_date, affordability_end_date,
  latitude, longitude, census_tract, cbsa_code, hud_region_name, rural_indicator,
  source_url, scrape_date, dataset_last_updated, update_frequency, data_license, crawler_run_id, version_hash
"""

class HUDCrawler:
    def __init__(self, dataset_path: Optional[str] = None):
        self.dataset_path = dataset_path
        self.raw = None
        self.df = pd.DataFrame()

    def load(self, path: Optional[str] = None) -> pd.DataFrame:
        """Load CSV or Excel into raw dataframe."""
        p = path or self.dataset_path
        if p is None:
            raise ValueError("You must provide a path to HUD dataset (CSV or XLSX).")
        if p.lower().endswith(('.xls', '.xlsx')):
            self.raw = pd.read_excel(p, dtype=str)
        else:
            self.raw = pd.read_csv(p, dtype=str)
        return self.raw

    def normalize(self, df: Optional[pd.DataFrame] = None) -> pd.DataFrame:
        """Normalize a HUD general dataset into a canonical set of columns."""
        if df is None:
            df = self.raw
        if df is None:
            raise ValueError("No data loaded to normalize.")
        # Normalize data column names to lower-case and strip
        cols_map = {c: c.strip() for c in df.columns}
        df = df.rename(columns=cols_map)
        # Generic required data attribute fields (must create if missing)
        required = [
            "dataset_name","fiscal_year","state_name","state_code","county_name","fips_code","hud_geoid","msa_code","area_type",
            "ami_median_family_income","ami_30_percent_limit","ami_50_percent_limit","ami_80_percent_limit","household_size",
            "program_type","sub_program","total_units","occupied_units","vacancy_rate","assisted_units","funding_source",
            "property_id","property_name","property_address","property_type","construction_year","owner_entity","managing_agent",
            "hud_inspection_score","last_inspection_date","affordability_end_date",
            "latitude","longitude","census_tract","cbsa_code","hud_region_name","rural_indicator",
            "source_url","scrape_date","dataset_last_updated","update_frequency","data_license","crawler_run_id","version_hash"
        ]
        for col in required:
            if col not in df.columns:
                df[col] = None

        # Type conversions for numeric fields where possible to avoid error handling
        numeric_cols = ["ami_median_family_income","ami_30_percent_limit","ami_50_percent_limit","ami_80_percent_limit",
                        "total_units","occupied_units","vacancy_rate","assisted_units","hud_inspection_score","construction_year"]
        for c in numeric_cols:
            df[c] = pd.to_numeric(df[c], errors='coerce')

        # dates
        df["last_inspection_date"] = pd.to_datetime(df["last_inspection_date"], errors='coerce')
        df["dataset_last_updated"] = pd.to_datetime(df["dataset_last_updated"], errors='coerce')

        self.df = df[required].copy()
        return self.df

    def filter_by_county(self, county_name: str) -> pd.DataFrame:
        if self.df.empty:
            raise ValueError("Data not normalized. Call load() and normalize() first.")
        return self.df[self.df['county_name'].str.contains(county_name, case=False, na=False)]

    def summary(self) -> Dict:
        if self.df.empty:
            return {}
        return {
            "rows": len(self.df),
            "unique_counties": int(self.df["county_name"].nunique()),
            "unique_programs": int(self.df["program_type"].nunique())
        }